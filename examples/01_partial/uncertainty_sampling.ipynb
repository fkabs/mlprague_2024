{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlocking the Power of Active Learning: A Hands-on Exploration\n",
    "\n",
    "Fabian Kovac [\\<fabian.kovac@fhstp.ac.at\\>](mailto:fabian.kovac@fhstp.ac.at)\n",
    "\n",
    "Oliver Eigner[<oliver.eigner@fhstp.ac.at\\>](mailto:oliver.eigner@fhstp.ac.at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets._samples_generator import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set SEED\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples to query at each iteration\n",
    "N_SAMPLES = 20\n",
    "\n",
    "# number of active learning iterations\n",
    "N_QUERIES = 100\n",
    "\n",
    "# percentage of human labeling error\n",
    "HUMAN_ERROR = 0.05\n",
    "\n",
    "# model to test (support vector machine with radial basis function kernel)\n",
    "MODEL = SVC(kernel = 'rbf', C = 13, gamma = 0.8, probability = True, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib styles\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (7, 4)\n",
    "plt.rcParams['image.cmap'] = 'Dark2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting svc decision boundaries\n",
    "def plot_svc_decision_function(model, ax = None, plot_support = False):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    x = np.linspace(xlim[0], xlim[1])\n",
    "    y = np.linspace(ylim[0], ylim[1])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "\n",
    "    ax.contour(X, Y, P, colors = ['crimson'], levels = [0], alpha = 1, linestyles = ['-'])\n",
    "    \n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s = 300, lw = 1, facecolors = 'crimson', alpha = 0.2)\n",
    "        ax.contour(X, Y, P, colors = ['crimson', 'crimson'], levels = [-1, 1], alpha = 0.3, linestyles = ['--', '--'])\n",
    "        \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting active learning history\n",
    "def plot_history(history, X_train, X_test, y_test, centralized_test_acc, show_models = False):\n",
    "    # plot each model state in the active learning history\n",
    "    # plotting decision boundaries is only implemented for SVC models for now\n",
    "    if show_models and type(history[0]['model']) == SVC:\n",
    "        for i, ret in history.items():\n",
    "            model = ret['model']\n",
    "            X_i = ret['X']\n",
    "            y_i = ret['y']\n",
    "            \n",
    "            # model metrics\n",
    "            train_acc = round(model.score(X_i, y_i), 3)\n",
    "            test_acc = round(model.score(X_test, y_test), 3)\n",
    "            \n",
    "            # generate two subplots, one for the training data (active learning iteratin) and one for the test data\n",
    "            fig, ax = plt.subplots(1, 2, figsize = (14, 4))\n",
    "            fig.suptitle(f'Active Learning Model', fontsize = 16, fontweight = 'bold')\n",
    "            plt.subplots_adjust(top = 0.8)\n",
    "\n",
    "            ax[0].title.set_text(f'Training Iteration {i} | Acc: {train_acc}')\n",
    "            ax[0].scatter(X_train[:, 0], X_train[:, 1], c = 'lightgray', alpha = 1, s = 40)\n",
    "            ax[0].scatter(X_i[:, 0], X_i[:, 1], c = y_i, s = 40)\n",
    "            plot_svc_decision_function(model, ax[0])\n",
    "\n",
    "            ax[1].title.set_text(f'Test data | Acc: {test_acc}')\n",
    "            ax[1].scatter(X_test[:, 0], X_test[:, 1], c = y_test, s = 40)\n",
    "            plot_svc_decision_function(model, ax[1])\n",
    "            \n",
    "            plt.show()\n",
    "    \n",
    "    # linegraph of model accuracy over iterations compared to centralized model\n",
    "    plt.axhline(y = centralized_test_acc, color = 'gray', linestyle = '--', label = 'Centralized Test Accuracy')\n",
    "    \n",
    "    test_accs = [ret['model'].score(X_test, y_test) for ret in history.values()]\n",
    "    plt.plot(test_accs, label = 'Active Learning Test Accuracy')    \n",
    "    \n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearner:\n",
    "    def __init__(self, X, y, model, n_samples, n_queries, strategy, human_error = 0.0):\n",
    "        # Training data and labels\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # Active learning model\n",
    "        self.model = model\n",
    "        self.model_init = deepcopy(self.model)\n",
    "        \n",
    "        # Number of samples to be queried at each iteration\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        # Number of Active Learning iterations\n",
    "        self.n_queries = n_queries\n",
    "        \n",
    "        # Active learning strategy\n",
    "        self.strategy = strategy\n",
    "        assert self.strategy in ['random', 'least_conf', 'margin_conf', 'ratio_conf', 'entropy'], 'Invalid strategy'\n",
    "        \n",
    "        # Human error\n",
    "        self.human_error = human_error\n",
    "        \n",
    "        # Data pools for active learning\n",
    "        self.X_pool = deepcopy(self.X)\n",
    "        self.y_pool = deepcopy(self.y)\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset model\n",
    "        self.model = deepcopy(self.model_init)\n",
    "        \n",
    "        # Reset data pools\n",
    "        self.X_pool = deepcopy(self.X)\n",
    "        self.y_pool = deepcopy(self.y)\n",
    "    \n",
    "    def _query(self, n_samples, init = False):\n",
    "        def _get_labels(idx):\n",
    "            # We simulate human labeling by correctly labeling the samples based on true labels\n",
    "            y_query = self.y_pool[idx]\n",
    "            \n",
    "            # Simulate human labeling error by randomly changing labels\n",
    "            if self.human_error > 0:\n",
    "                idx_mask = np.random.rand(len(y_query)) < self.human_error\n",
    "                y_query[idx_mask] = np.random.choice(np.unique(self.y), size = np.sum(idx_mask), replace = True)\n",
    "            \n",
    "            return y_query\n",
    "        \n",
    "        if init:\n",
    "            # Randomly select samples for initial labeling\n",
    "            idx = np.random.choice(range(len(self.X_pool)), size = n_samples, replace = False)\n",
    "            \n",
    "            # Make sure, that all classes are present\n",
    "            while len(np.unique(self.y_pool[idx])) < len(np.unique(self.y)):\n",
    "                idx = np.random.choice(range(len(self.X_pool)), size = n_samples, replace = False)\n",
    "        else:\n",
    "            # Predict probabilities for each class\n",
    "            y_prob = self.model.predict_proba(self.X_pool)\n",
    "        \n",
    "            if self.strategy == 'random' or init:\n",
    "                # Randomly select samples\n",
    "                idx = np.random.choice(range(len(self.X_pool)), size = n_samples, replace = False)\n",
    "                \n",
    "                # If init, make sure, that all classes are present\n",
    "                if init:\n",
    "                    y_unique = len(np.unique(self.y_pool[idx]))\n",
    "                    while y_unique < len(np.unique(self.y)):\n",
    "                        idx = np.random.choice(range(len(self.X_pool)), size = n_samples, replace = False)\n",
    "                        y_unique = len(np.unique(self.y_pool[idx]))     \n",
    "            elif self.strategy == 'least_conf':\n",
    "                # Difference between the most confident prediction and 100% confidence\n",
    "                most_conf = np.nanmax(y_prob, axis = 1)\n",
    "                numerator = (y_prob.size * (1 - most_conf))\n",
    "                denominator = (y_prob.size - 1)\n",
    "                idx = np.argpartition((numerator / denominator), -n_samples)[-n_samples:]\n",
    "            elif self.strategy == 'margin_conf':\n",
    "                # Difference between the top two most confident predictions\n",
    "                raise NotImplementedError('Margin Confidence sampling strategy is not implemented yet ,-)')\n",
    "            elif self.strategy == 'ratio_conf':\n",
    "                # Ratio between the top two most confident predictions\n",
    "                raise NotImplementedError('Ratio Confidence sampling strategy is not implemented yet ,-)')\n",
    "            elif self.strategy == 'entropy':\n",
    "                # Difference between all predictions based on entropy, as defined by information theory\n",
    "                raise NotImplementedError('Entropy sampling strategy is not implemented yet ,-)')\n",
    "        \n",
    "        # Retrieve queried samples from data pool\n",
    "        X_query = self.X_pool[idx]\n",
    "        \n",
    "        # Get human labeled data based on queried samples\n",
    "        y_query = _get_labels(idx)\n",
    "        \n",
    "        # Remove queried samples from data pool\n",
    "        self.X_pool = np.delete(self.X_pool, idx, axis = 0)\n",
    "        self.y_pool = np.delete(self.y_pool, idx, axis = 0)\n",
    "            \n",
    "        return X_query, y_query\n",
    "    \n",
    "    def fit(self):\n",
    "        # Reset model and data pools\n",
    "        self.reset()\n",
    "        \n",
    "        # History of models\n",
    "        history = {}\n",
    "        \n",
    "        # Fit model on randomly queried training data (simulates pre-labeled data)\n",
    "        X_train, y_train = self._query(self.n_samples, init = True)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        history.update({\n",
    "            0 : {\n",
    "                'model': deepcopy(self.model),\n",
    "                'X': X_train,\n",
    "                'y': y_train\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Active learning iterations\n",
    "        for i in range(self.n_queries):\n",
    "            # Check if enough samples are left for labeling\n",
    "            if len(self.X_pool) == 0:\n",
    "                break\n",
    "            elif len(self.X_pool) < self.n_samples:\n",
    "                n_samples = len(self.X_pool)\n",
    "            else:\n",
    "                n_samples = self.n_samples\n",
    "            \n",
    "            # Query samples based on selected strategy\n",
    "            X_query, y_query = self._query(n_samples)\n",
    "            \n",
    "            # Add queried samples to training data\n",
    "            X_train = np.concatenate((X_train, X_query))\n",
    "            y_train = np.concatenate((y_train, y_query))\n",
    "            \n",
    "            # Fit model on queried samples\n",
    "            self.model.fit(X_train, y_train)\n",
    "            history.update({\n",
    "                i+1 : {\n",
    "                    'model': deepcopy(self.model),\n",
    "                    'X': X_train,\n",
    "                    'y': y_train\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Return model\n",
    "        return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X, y = make_classification(\n",
    "    n_samples = 1000, n_features = 2, n_classes = 2,\n",
    "    n_informative = 2, n_redundant = 0, n_clusters_per_class = 2, class_sep = 0.85,\n",
    "    random_state = SEED)\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = SEED)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, s = 40);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Centralized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a centralized model\n",
    "centralized_model = deepcopy(MODEL)\n",
    "centralized_model.fit(X_train, y_train)\n",
    "\n",
    "# model metrics\n",
    "train_acc = round(centralized_model.score(X_train, y_train), 3)\n",
    "test_acc = round(centralized_model.score(X_test, y_test), 3)\n",
    "\n",
    "# plot decision boundaries\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14, 4))\n",
    "fig.suptitle(f'Centralized model', fontsize = 16, fontweight = 'bold')\n",
    "plt.subplots_adjust(top = 0.8)\n",
    "\n",
    "ax[0].title.set_text(f'Training data | Acc: {train_acc}')\n",
    "ax[0].scatter(X_train[:, 0], X_train[:, 1], c = y_train, s = 40)\n",
    "plot_svc_decision_function(centralized_model, ax[0])\n",
    "\n",
    "ax[1].title.set_text(f'Test data | Acc: {test_acc}')\n",
    "ax[1].scatter(X_test[:, 0], X_test[:, 1], c = y_test, s = 40)\n",
    "plot_svc_decision_function(centralized_model, ax[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Active Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset seed to ensure same initial model\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Active Learning using Random Sampling\n",
    "learner = ActiveLearner(\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    model = deepcopy(MODEL),\n",
    "    n_samples = N_SAMPLES,\n",
    "    n_queries = N_QUERIES,\n",
    "    strategy = 'random',\n",
    "    human_error = HUMAN_ERROR\n",
    ")\n",
    "\n",
    "history = learner.fit()\n",
    "\n",
    "plot_history(history, X_train, X_test, y_test, test_acc, show_models = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Confidence Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset seed to ensure same initial model\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Active Learning using least confidence sampling\n",
    "learner = ActiveLearner(\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    model = deepcopy(MODEL),\n",
    "    n_samples = N_SAMPLES,\n",
    "    n_queries = N_QUERIES,\n",
    "    strategy = 'least_conf',\n",
    "    human_error = HUMAN_ERROR\n",
    ")\n",
    "\n",
    "history = learner.fit()\n",
    "\n",
    "plot_history(history, X_train, X_test, y_test, test_acc, show_models = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin Confidence Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset seed to ensure same initial model\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Active Learning using margin confidence sampling\n",
    "learner = ActiveLearner(\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    model = deepcopy(MODEL),\n",
    "    n_samples = N_SAMPLES,\n",
    "    n_queries = N_QUERIES,\n",
    "    strategy = 'margin_conf',\n",
    "    human_error = HUMAN_ERROR\n",
    ")\n",
    "\n",
    "history = learner.fit()\n",
    "\n",
    "plot_history(history, X_train, X_test, y_test, test_acc, show_models = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio Confidence Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset seed to ensure same initial model\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Active Learning using ratio confidence sampling\n",
    "learner = ActiveLearner(\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    model = deepcopy(MODEL),\n",
    "    n_samples = N_SAMPLES,\n",
    "    n_queries = N_QUERIES,\n",
    "    strategy = 'ratio_conf',\n",
    "    human_error = HUMAN_ERROR\n",
    ")\n",
    "\n",
    "history = learner.fit()\n",
    "\n",
    "plot_history(history, X_train, X_test, y_test, test_acc, show_models = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset seed to ensure same initial model\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Active Learning using entropy sampling\n",
    "learner = ActiveLearner(\n",
    "    X = X_train,\n",
    "    y = y_train,\n",
    "    model = deepcopy(MODEL),\n",
    "    n_samples = N_SAMPLES,\n",
    "    n_queries = N_QUERIES,\n",
    "    strategy = 'entropy',\n",
    "    human_error = HUMAN_ERROR\n",
    ")\n",
    "\n",
    "history = learner.fit()\n",
    "\n",
    "plot_history(history, X_train, X_test, y_test, test_acc, show_models = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
